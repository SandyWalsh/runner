# runner design doc

## Background
This repo is intended to hold the client, server (api), and underlying (library) golang code for a gRPC-based runner of linux commands. 

(phew ... that's a mouthful)

Not only that, but it should be secured by TLS, honor cgroups and, optionally, linux namespaces, and stream data back to the client from the running process. 
Impossible you say?! Maybe. Think of it as making docker runtime without the image management.

This document will outline my approach to make this a reality. 

### Broad strokes

I hope to implement this in a series of PR's that follow this schedule:

0. this doc
1. create the library for the process runner and cgroup/ns support (perhaps based on client id roles)
2. basic TLS-secured gRPC client-server chat. The API will wrapper the process runner library
3. support for hard coded gRPC streaming data. Something simple like a clock tick
4. Spawn some basic jobs, capture stderr/stdout and pump them into the streams
5. Work on job control. Cancelling a process, ensuring the channels/goroutines/resources clean up correctly, etc. 
6. Add cgroups and resource isolation 

(this may change)

Stretch goals

7. some key tests
8. docker packaging
9. a nice client
10. magic

## API

Exposed gRPC "Runner" service methods:

```
Run(ClientID, cmd, args...) (Process, error)
GetStream(ClientID, Process) (Stream, error)
Status(ClientID, Process) (Status, error)
Abort(ClientID, Process) (Status, error)
```

where: 
 * the `Run` method that can launch new processes for authenticated callers.
   * the "role" of the authenticated client will determine the cgroup style the process runs in. This will be extracted from client mTLS cert.
   * `Run` takes `cmd` and `args` args for the intended linux command and related cmdline parameters and returns a `Process` message
 * `Process` is an abstracted process ID (aka `job_id`) for the process control calls: `GetStream`, `Status`, and `Abort`. It will be a UUID that maps to the underlying os pid. 
 * `Status` will return an enum of `COMPLETED|RUNNING|ERROR|ABORTED` and, ideally, the exit code

## AuthN / AuthZ

mTLS client certs will be used for authN. The client cert will map to a set of "roles" (via the `UID` x.509 field) that determines the class of cgroup processes will run under. For authz a simple check that the client `UID` has an entry in the client->assigned_cgroup mapping table will determine if they can launch a process or not. We could extend this such that only some roles can abort a process or attach to it. 

| cgroup template | CPU | Memory | Disk IO | 
| ------------| --- | ------ | ------ |
| **good**    | x   | x      | x      |
| **better**  | x   | x      | x      |
| **best**    | x   | x      | x      |

| client | assigned cgroup | 
| -------- | --------------- |
| **cert-A** | good            |
| **cert-B** | better        |
| **cert-C** | best            |


## Client UX

The client initially will just be a golang program that runs the server through some paces (just an integration test harness). But in later releases I will try to turn this in to a simple cmdline tool with basic support for each operation. 

```
> client -cert cert.pem run ls -laR
aaaa-bbbb-ccc-ddd-eeee
> client stdout -cert cert.pem -job aaaa-bbbb-ccc-ddd-eeee
.
..
README.md
> client status -cert cert.pem -job aaaa-bbbb-ccc-ddd-eeee
COMPLETED
Exit code: 0
> client abort -cert cert.pem -job aaaa-bbbb-ccc-ddd-eeee
already terminated
```

Beyond the raw client code generated by the gRPC code there will not be a separate "sugar" client library. Although it would be handy for managing the stdout streams. 

## Implementation tradeoffs & security considerations

### Thread safe, Multi-Reader `bytes.Buffer`

`bytes.Buffer` is not thread safe and does not support concurrent readers. I'll have to implement a compatible implementation that supports these two features. 
I'll refer to it as `MRBuffer` 

### Running processes

1. Client makes a mutual mTLS connection to server. 
1. Server will use `UID` field of client's X.509 cert to determine the client -> role mapping (good, better, best)
1. Server will create cgroups related to the role of the client
1. Server will launch process with namespace `SysProcAttr`'s related to the role of the client. `stderr` and `stdout` will be sent to `MRBuffer`
1. NOTE: Server will not run the process directly. Due to a race condition with starting the process and adding the pid to the `cgroup.procs` file we will use a small bash script that starts and immediately pauses itself (via `SIGSTOP`). 
1. A UUID will be created that maps to the process os pid. This will be returned to the caller. 
1. The pid will added to the `cgroup.procs` file
1. Server will `SIGCONT` the wrapper process which will shell `exec` the callers intended process (keeping the original pid)
1. The Server will `Wait` for the process to end (or be terminated). 
1. Process state will be tracked via the golang server state and not by calling the os for process status
1. When the process terminates the related cgroup will be torn down
1. If the caller decides to stream the process output, a new `Reader` will be spawned from `MRBuffer`and sent to the grpc stream.

### Shortcuts

* Long lived gRPC services can be tricky with streaming messages. I will be ignoring all the retry/re-establish connection code. 
* All the authenticated clients will have hardcoded "roles" which map to cgroups. 
* I may limit cgroup control to just the cpu controller, memory, and disk I/O. 
* "domain" cgroup types will be used. "threaded" won't be supported. 
* The Subject Alternative Name (SAN) on the certificates will be localhost and not 0.0.0.0

## Security
The CA and server cert will be generated with openssl and based on a 4096 bit RSA key pair. 
An X.509 cert will be fabricated from these key pairs.
Client certs will also be X.509 as above. 
All certs will be created without passphrases. 

There are a number of security concerns with such a service:
* in order to exec commands the process will need to run privileged system calls. This requires root level access. Which means a bad actor could basically do anything. We'll need to use cgroups and resource isolation to protect against this. 
* Setting the file permissions on the cgroup v2 files will be important for limiting child processes from manipulating the cgroup settings. 
* the uid/gid for the user to run the processes as will be hardcoded
* The TLS certs will be stored in source control. Ideally these should be in a good secret manager and merged at runtime. 

## Sample ControlGroup and Authz structure
When initializing the runner library we need to pass in the authz rules that are applied when a caller attempts to run a process. 
These authz rules are a collection of cgroup values and, eventually, `SysProcAttr` values. 

In the sample `AuthzRules` structure below, we are defining three styles of cgroup and the related settings. Additionally we define the caller cert `UID` to role mapping. 

NOTE: cores, processors, memory capacity and sta devices will need to be queried from you target system. These values are just examples. 

```
	good := library.ControlGroup{
		Name: "good",
		Limits: []library.Limit{
			{Var: "cpuset.cpus", Value: "1"},
			{Var: "cpu.max", Value: "100000 1000000"},
			{Var: "cpu.weight", Value: "50"},
			{Var: "memory.max", Value: "250M"},
			{Var: "io.max", Value: "259:0 rbps=2097152 wiops=120"},
		},
		SysProcAttr: &syscall.SysProcAttr{
			Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID,
		}}

	better := library.ControlGroup{
		Name: "better",
		Limits: []library.Limit{
			{Var: "cpuset.cpus", Value: "1"},
			{Var: "cpu.max", Value: "250000 1000000"},
			{Var: "cpu.weight", Value: "100"},
			{Var: "memory.max", Value: "500M"},
			{Var: "io.max", Value: "259:0 rbps=4194304 wiops=240"},
		},
		SysProcAttr: &syscall.SysProcAttr{
			Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID,
		}}

	best := library.ControlGroup{
		Name: "best",
		Limits: []library.Limit{
			{Var: "cpuset.cpus", Value: "1"},
			{Var: "cpu.max", Value: "500000 1000000"},
			{Var: "cpu.weight", Value: "150"},
			{Var: "memory.max", Value: "1G"},
			{Var: "io.max", Value: "259:0 rbps=8388608 wiops=360"},
		},
		SysProcAttr: &syscall.SysProcAttr{
			Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID,
		}}

	authz := library.AuthZRules{
		ControlGroups:  []library.ControlGroup{good, better, best},
		ClientToCGroup: map[string]string{"cert-A": "good", "cert-B": "better", "cert-C": "best"},
	}

	runner := library.NewRunner(authz, ...
```